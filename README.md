Accurate forecasts of solar and wind generation help power systems keep supply and demand in balance. This interim report documents the scope, data, methods, exploratory data analysis (EDA), preliminary results, and next steps for a global, multi‑site project that produces 15‑minute, 1‑hour, and 24‑hour ahead forecasts using hybrid deep learning models and strong statistical baselines. Each processing step is explained in plain language—what was done, why it was done, what was expected, and what was observed—so a non‑specialist can follow the logic. The report integrates the uploaded notebooks, showing how raw multi‑source files were converted into a workable, model‑ready dataset. Sample‑size/power equations are included to justify that comparisons are statistically meaningful, and APA‑style references are provided to the literature that guided the approach.
Keywords: renewable energy, solar, wind, forecasting, multi‑horizon, deep learning, transfer learning, EDA, time series
1. Introduction
Renewable generation is variable. Solar output follows the sun and cloud patterns; wind output fluctuates with boundary‑layer dynamics, storms, and terrain. Operators must plan minutes ahead (ramp control), hours ahead (unit commitment), and day‑ahead (market bids, storage scheduling). Much prior research emphasizes short‑term horizons (1–6 hours), which are valuable for operations but insufficient for day‑ahead planning. The present study addresses this gap by building a single pipeline that delivers forecasts at multiple horizons (15‑minute, 1‑hour, and 24‑hour) from the same cleaned dataset.
This work advances three practical goals. First, a reproducible global data pipeline is demonstrated: open meteorology and irradiance sources are harmonized into one tidy table per site and then merged into region‑level and global panels. Second, hybrid deep learning models (CNN‑LSTM/TCN) are compared against strong baselines (persistence, seasonal naïve, ARIMA/SARIMA, XGBoost) under time‑aware cross‑validation, so improvements are credible. Third, the study quantifies how accuracy degrades as the horizon lengthens (RQ1) and whether deep models outperform baselines (RQ2). Explanations are kept simple, linking steps to the expected outcome and recording what occurred.
Contributions. (i) A documented global multi‑site dataset build; (ii) a transparent EDA that reveals physical relations (e.g., solar↔GHI, wind↔hub‑height speed) by using daylight and non‑calm filters; (iii) a disciplined validation design with baseline comparators; and (iv) sample‑size/power formulas that translate “practical improvements” (e.g., 5% MAE reduction) into data requirements.
2. Scope and Objectives
Scope- The dataset includes multiple global sites across Europe, the United States, Australia, and Asia, pulled from open sources (ERA5, NSRDB/PSM3 where available, PVGIS‑style irradiance, ENTSO‑E/OPSD‑style targets, and Global Renewables Watch for asset context). Targets include hourly solar and wind generation (site, regional, and country levels). Outputs differ by geography due to climate and terrain.
Horizons and cadence- Forecasts are produced for 15‑minute, 1‑hour, and 24‑hour ahead from an hourly base cadence; where sub‑hourly data exist, aggregation or down‑sampling is applied with care.
Objectives-
O1. Measure how forecast accuracy changes with the horizon at single‑site and global multi‑site levels.
O2. Test whether hybrid deep models beat persistence, seasonal naïve, ARIMA/SARIMA, and XGBoost across geographies.
O3. Identify the most important drivers (irradiance, hub‑height wind vectors, pressure, cloud cover, seasonality) and check if they differ by location.
O4. Assess model transferability across countries/continents via transfer learning (full results deferred to the final report; methods prepared here).
Research Questions and Hypotheses (interim focus)
RQ1. How does forecast accuracy change across time horizons (15 minute, 1 hour, 24 hour)?
H0 (RQ1). Mean error does not change with horizon: MAE_15min = MAE_1h = MAE_24h (and similarly for RMSE/nMAE, within a small tolerance epsilon).
H1 (RQ1). Mean error worsens as the horizon increases, with the expected ordering MAE_15min < MAE_1h < MAE_24h; the largest deterioration is typically from 1 hour to 24 hour.
Tests. Blocked time series cross validation; paired fold level comparisons of horizons using Wilcoxon and sign tests, with effect sizes and bootstrap CIs; power assessed per Section 9.
RQ2. Do hybrid deep learning models give better results than traditional models?
H0 (RQ2). There is no improvement versus the strongest baseline (persistence or seasonal naïve/ARIMA as appropriate): for each site/fold/horizon, the paired difference Δ = MAE model − MAE_best_baseline has median ≥ 0.
H1 (RQ2). Modern ML and hybrid deep models (e.g., CNN LSTM/TCN/TFT) achieve lower errors than the strongest baseline across sites and horizons: median Δ < 0, with a practically meaningful reduction (for example, at least 5% of baseline MAE).
Tests. Blocked CV with a gap; paired Wilcoxon and sign tests across folds/sites; effect sizes; multiple comparison control (Holm); and power checks as in Section 9.
RQ3. Which input features impact forecasts the most and why? (Analysis to be reported in the final report, but hypotheses are fixed now.)
H0 (RQ3). After controlling for other variables and seasonality, no candidate driver has materially greater predictive contribution than the rest: distributions of absolute SHAP values are equal across features and are not larger than those of noise/decoy features; ablating any single driver changes MAE by < δ (δ = 1% of baseline MAE).
H1 (RQ3). For solar, irradiance (GHI/DNI/DHI) and cloud cover have significantly larger absolute SHAP values than other features; for wind, 100 m wind speed (and air density/wind vector terms) dominate. Importance profiles differ by region/continent.
Tests. SHAP value comparisons via permutation tests/Wilcoxon rank sum with Holm adjustment; bootstrap rank stability; ablation tests (drop one driver) assessing ΔMAE with δ = 1% threshold per horizon/site.
